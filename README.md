# Intro

The lab demo is meant to show / help with sample setup, not necessarily the most correct one when it comes to the way BGP is configured or routes are propagated

![Lab Topology](https://user-images.githubusercontent.com/43679669/190932186-dce3cc02-6031-47ca-affc-ee9d7d4f740f.png)

# Application Environment Overview

## Launch the lab environment

In this section, we will launch the lab environment. These are the steps that we will accomplish at this time.

- Start the lab on your designated Qwiklab account.
- Login to the AWS Console using the provided credentials and set up  IAM roles
- Subscribe to the Panorama appliance on the AWS Marketplace.
- Deploy EKS environment using Terraform

## Start Qwiklabs lab environment and login to AWS

1. Once you login to [paloaltonetworks.qwiklabs.com](https://paloaltonetworks.qwiklabs.com/), the Home page should display the Labs that you have access to. Identify and click on the Lab that says "_Professional Services - CN-Series Workshop_".

2. On the page that opens up, click on _Professional Services - CN-Series Workshop - CNF on EKS_.

3. On the Qwiklab environment, Click on _Start Lab_ Button to start the lab.

At this point, Qwiklabs will build an AWS account for you. In order to access the EC2 application instances via SSH in your environment, you will be using keys generated by Qwiklabs. There are two types of keys generated; PEM and PPK keys.

1. If you are on a MAC, you will be using ‘Terminal’ to connect to the devices via SSH. For this, click on the “Download PEM” link. This will download a file named “qwikLABS-L\*\*\*\*\*-\*\*\*\*\*.pem”.

   - Make sure to note the location where the file is downloaded. On a Mac, by default, it will be downloaded to “/Users/&lt;username>/Downloads”

2. If you are using a windows laptop to access this lab, you will need to have a ssh application like PuTTY installed. 
![](https://lh4.googleusercontent.com/KztKlSkNNaQU61dPFGIAxrgDK8OSeSy6SGbcfPaO4Yji-EzBR_leZ02KN3u9_Twcj1qVL5B0OYOD_hZt2z1YWT5NQI53PzHl4HOzJAGxMz5EdedRkG3z62p7aouOzV-bAyWY3cDPJBwdkLrrpw)

   - In this case, click on the “Download PPK” link. This will download a file named “qwikLABS-L\*\*\*\*\*-\*\*\*\*\*.ppk”.

![](https://lh4.googleusercontent.com/9mJrciMHAaP929OpmukkN2K7nJZAEWqHY9tiAOaAdJZPeMc7My1tTRJKj3ZNhbfKWbTcvY8gBjfkQ0Bhxlbxy3eptSvk_CeFIrdugaI8MYkyz_g_d6McJocu7mtDxymFRbvpoEgS9fqwya0Ipw)

6. To login to the AWS environment, right click on “Open Console’ and “Open link in Incognito window” for Chrome-based browsers. For other browsers, use the appropriate option to open the AWS Console in a new private tab.

![](https://lh6.googleusercontent.com/uT9aD3yPqEH281H3ArxEeD__pjYDaJMSyv6CN0Fe78cnAjkNa_TA5LMKqHw3hUNUkWGI-bO3YuxXSuehWHvbIPR-EL616pesbcCxdLDkoeYrApT3AuvRXXPazBH1TIIl4IUGUXGL0mO4faL2aSc)

7. On the AWS Console, copy over the IAM username and password from the previous tab.

![](https://lh5.googleusercontent.com/b9Wwy3UiNDKna9cJkaV0SvyP9YFhIJV9Q0t_mqzvjRlo_RUNXNlqxxhZ1gDOen_RTkMxQQSoLRG3QIXT7GqvnzbkgpxrqBASd8Be1ZSVas_o5DNtwfmcjnfhwwfysDITt2KHxhC6W33pZEiykg)![](https://lh4.googleusercontent.com/fpGosvriB6Z2pqvMM70L7xUCHEpmfj55-1_BaPR_pDIBxprGXVi_v_YZr1aY4kc1IFUIz7rRoELUhNFDUT51EHGSVw326zeDCBnhvd6DbpMlrvEgyIqWUMpCLY_SIbBtro2au3GYk8CetXrTlw)

8. Now, click on “Sign In”.

![](https://lh5.googleusercontent.com/J5se3uljpa6L3MK-TVqMxBBplVKQQnZ6Wrh5atg3h_RiYAZtJZgaqSAe4-uV5RCk7h3bQ-QrUSQMvOAKBDKjS6kQ4iILWerNNnnfkDN4ryx5WB1DxlyrpQyLCLhJfg_CjH98fwkre31l2hurVQ)

Once you are successfully logged in, you will land on the AWS Management Console.

![](https://lh6.googleusercontent.com/vLh-4_cDPwBtDGcH5Vq08ai8wCwVzRY_zNsCn8fxhKL493RamNO5PrZWNbxC8d54OXx1_FbVCb6z1uesMBE9kI7DW33NR31JHxIqAxCqR-_19vNYtX_uNtxasgVhDNIlfJPpVEozEwyzZcQSag)
**Figure:** The AWS Management Console

## Set up AWS Account permissions

The Qwiklab user account, by default, does not have the permissions to AWS Marketplace and CloudShell services, which are required for the purpose of this lab. We will now edit the permissions for the Qwiklab user account to provide access to those services.

On the AWS console,

9. If you see a message for ‘new AWS Console’ click on ‘Switch now’
10. On the search bar type ‘iam’.
11. Click on the link to _IAM_. A new IAM dashboard window will open.

![](https://lh6.googleusercontent.com/prkjUS9HYswz-jjKU1lmwO6vUu5NzB-ffyeWv0hpQcnmmWFpXWtPo0pSCn90Jig1xfa6paud-ITotC500osJpEiVR1FtRmhMa5RznNJOL1pd2rceW_eBCts2zRGBWmyzOANZJrPuSIMyKmr1MQ)

12. Click on ‘2’ below users.

![](https://lh3.googleusercontent.com/6i-i100jVqCcVvg9eA6QBzy6CBTU2Q0qGoc_4vjt7X7r7AlojhYW4h84aiTeWTym5UU6hLQBtKE0o9CPudc5RW6edyylGxMl4WNbupQQwVSZMXjRy83BxBUrZsVdS7FglsS6reBkdpL6Wqimcw)

13. Click on ‘awsstudent’.

![](https://lh3.googleusercontent.com/rCDrl8lHLPx-XWCqcAe5o1IA_-N1fVLHwod7pSxLc_wzPbWh2LP_m-Ipzi9Vc4RST-3NbqqBbIr6Q2_ggKLUsEPlTBt3jACeCiDvlNkj_xtakraRHo60hDInJpFoIz4Tv2qehfnnADUsThc3_g)

14. Expand the default policy by clicking on the small triangle icon against default_policy in the list.

![](https://lh5.googleusercontent.com/DoxiUC4CM9Mt8HH2oIBS1v6jf2mUnsQmIaiZoYbY6k8A0HjLVDm5rjVXWLOpZZ_c73jGpOVqq1jXINOT1ai8nR7H8ubd5EWkSTeaYRSmt1o9pOd7DxiQjCkM_JaF8B0g9JT2Rtz2SeavwIhJNA)

15. Click on the ‘Edit Policy’ button.

![](https://lh4.googleusercontent.com/VtZe2xx-WurO_LpxfiRcRPkPgZZrWv2x0TU-2MhhDEa1OmtoucFVhC9KoIRL-jLF7Re9SouK3LbRfkzJgyeJ9Y8FA93fuF30j13iR2cSvpDFJTxjc9q0aYkLFUFcpRO-rg5zJa7NMtwBp3OP_w)

16. Click the ‘JSON’ tab.

![](https://lh3.googleusercontent.com/o44YVIRRJZkh4Q27BgCtAclylZRMrlXKvRlVoZ6JmQoSaWvTV_L6gMMUEB038lVRobt-M5xiir8OVr6jW3tCH-KW85hb0LEdnOMNv-XQbR-VniOhV8_Mh91ZBfCfI5f8Sff0ys0uP1YyCDL0ww)

17. Scroll down to the Deny policy and remove two lines (**line number 27** and **line number 36**) listed below

```
"aws-marketplace:\*ubscribe",
...
"cloudshell:\*",
```

Make sure to delete the whole line.

![](https://lh6.googleusercontent.com/gYkT0FH78QpnP24zVZPpKgSdtDLf1qaqmMb0AIpHtJ_O1B1AtirKcG28AQ_GXedKM_eFjgN7-o_2ZEeRD7syK2fdYPN_X3OVvECY9y6FbLp75EsN3lVTGBM7ptpPZCLwvoYRupOOyoaTM9wNcg)

18. Click on ‘Review policy’ at the bottom of the screen.
19. On the next screen, click on ‘Save changes’.

![](https://lh6.googleusercontent.com/iZlrM6e0CPx6Wv6ylxAU9R1wUnpAjlEj697Nu6VOd-sCOqUwhERAOB_JMCDXiRHJn_4XIQvtV-OV07Q1G98dUnS45DcxJfmwdEM5c6vF4Illor73BBw9p7_6SEqFRQ_OFH5Ky9nbeBWKk-vobQ)

20. Account setup is now complete.


## Subscribe to Panorama on the AWS Marketplace

In this section, we will deploy the AWS Cloud resources required for the purpose of this lab using Terraform. 

21. Navigate to the [Marketplace Console](https://us-east-1.console.aws.amazon.com/marketplace/home?region=us-west-2#/)
22. Check if Panorama is already subscribed
23. If not, go to Discover Products and search for Panorama and subscribe

24. Wait till Effective date changes from ‘Pending’ to today’s date. This will take a few minutes.

![](https://lh3.googleusercontent.com/L1M7PgFTujgp-C5XtxfQWH0MJbWHZI6nfzR8JDWz8H4chMkuzxyh422ZdpIQFw8ODE8eJXz8J_lYK4mK_-0l62MBaxR_hoDDXr4tFvQ0E7bp6frc76c19EyrjLYFeuZDyNSxKYmnL15IK1fzhg)


## Launch and prep CloudShell Environment

In AWS CloudShell, anything saved in the home directory will persist for 90 days. Anything outside of the home directory will be wiped when the session times out. The AWS accounts for QwikLabs are re-used and the home directories are not automatically wiped.

**The CloudShell is unique per region. If you are trying to access data from CloudShell, make sure you select the region you initially used. For this lab, CloudShell from any region can be used but the resources will be deployed in the Oregon (us-west-2) region, so it will make sense to use that region**


25. Select the `Oregon` region from the top-right. (If you previously used `N. Virginia` region it is fine to continue using it)


25. From the AWS management Console, launch CloudShell using the icon on the top right side of the console.

If you do not see the icon as shown in the image above, please check the region and ensure that you are in an appropriate region.

26.  Close out the welcome pop up.

![](https://lh5.googleusercontent.com/xKRj8zULWZouUYTzXEuYkxGHgp1On1BuSxagxKI2c0hv7YY2cgDUzXa5Pis3nrFF1ZZqG-jnmpnB9b0nbkcZwW4b-6HiB3KjI4JMbHO5bf-clM9HnrYhQv12LZXSP9kW3cCd4c3z5kJJOuBfFw)

It takes around a minute for cloudshell to launch and to get the prompt as shown in the example below.

27. After the cloudshell is launched, we will first ensure that the home directory is empty by running the below command.

```
rm -rf ~/*
```

# AWS infrastructure

1. clone the repo
```
git clone https://github.com/seanyoungberg/ps-aws-lab-cnf-eks.git

```

2.  Prepare CloudShell environment

Script will install Terraform, kubectl, and aws-iam-authenticator

```
sh ~/ps-aws-lab-cnf-eks/setup.sh
```

3. Quality of life tweaks for working with `kubectl`

These are common setup steps for working with kubectl but they do not always work well with AWS CloudShell.

```
sudo yum install -y bash-completion
```

```
source <(kubectl completion bash) # setup autocomplete in bash into the current shell, bash-completion package should be installed first.

echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanently to your bash shell.
echo "alias k=kubectl" >> ~/.bashrc
echo "complete -o default -F __start_kubectl k}" >> ~/.bashrc
source ~/.bashrc
```


4. Change to terraform directory
```
cd ~/ps-aws-lab-cnf-eks/tf
```

5. download lambda function
```
curl -L -k https://github.com/aws-samples/eks-install-guide-for-multus/raw/main/cfn/templates/nodegroup/lambda_function.zip -O
```

6. Get the name of the SSH Key Pair that was generated by QwikLabs

```
aws ec2 describe-key-pairs | grep KeyName
```

7. Edit terraform.tfvars and add the name of the SSH Key Pair

```
key_pair = "<user>"
```

8. Terraform init and apply
```
terraform init
terraform apply
```

## jump host preparation

1. Identify the Public IP of the Jump Host in the EC2 Console

2. Copy the the kernel module binaries to jump host, replace the key and jump host IP accordingly
```
cd ~/ps-aws-lab-cnf-eks/
scp -i ${QWIKLABS-key.pem} bin/igb_uio.ko bin/rte_kni.ko ubuntu@${aws-cnv3-jump-public-ip}:
```
ssh to jump host and move the files to nginx web folder
```
ssh -i ${QWIKLABS-key.pem} ${aws-cnv3-jump-public-ip}
sudo mv igb_uio.ko rte_kni.ko /var/www/html/
```

## Setup multus and scale the cluster


1. Apply multus from CloudShell
```
kubectl apply -f https://raw.githubusercontent.com/aws/amazon-vpc-cni-k8s/master/config/multus/v3.7.2-eksbuild.1/aws-k8s-multus.yaml
```
2. Scale Up to launch EKS nodes
  
Autoscaling group was deployed with 0 nodes. Scale it up. Adjust the region and name as needed
```
aws autoscaling set-desired-capacity --region us-west-2 --auto-scaling-group-name rwe-cnv3-ng1 --desired-capacity 2
```

## Panorama

To bring up BGP peering with multus hosts deployed by TF you will need to create a template with the necessary VR/BGP/interface configuration. The cli commands creating these are in the *panorama_bgp_template.cli* file. 

1. Identify Panorama public IP from the EC2 Console

2. SSH to Panorama

3. Use Panorama CLI to apply set commands for the template settings from the *panorama_bgp_template.cli* file.

4. Generate VM Auth Key

- Generate a vm-auth-key that will be used for the CN-Series to bootstrap
- Record key for use later

```
request bootstrap vm-auth-key generate lifetime 8760
```
5. Create Template Stack named `ts_k8s_cnv3` that includes the template `cnv3bgp`

6. Create Device Group named `dg_k8s_cnv3`. You can leave it empty for now.


## Helm

**If you previously installed helm 3.9, you will need to remove it**
```
sudo rm /usr/local/bin/helm
```

1. Install Helm 3.8.2
```
sudo yum install openssl
wget https://get.helm.sh/helm-v3.8.2-linux-amd64.tar.gz
tar -zxvf helm-v3.8.2-linux-amd64.tar.gz
mv linux-amd64/helm ~/bin/
```

2. Verify you are using 3.8.2

```
helm version
```

3. Create the helm values file: `~/ps-aws-lab-cnf-eks/eks-h.yaml`. Update your Panorama authKey and public IP.

Hint - Try `:set paste` in vi to help with the paste
Hint - Otherwise create the file locally and then upload to cloudshell and move it to `~/ps-aws-lab-cnf-eks/eks-h.yaml`

```
---
common:
  cr: "us.gcr.io/panw-gcp-team-testing/paloaltonetworks"
  versionPanos: "10.2.2"
  versionInit: "3.0.2"
  #pullSecretName: gcr-json-key


panorama:
  authKey: "<Add Here>"
  ip: "<Add Here>"
  dg: dg_k8s_cnv3
  ts: ts_k8s_cnv3

dp:
  dpdk: true
  cpu: 1
  networks:
  - name: ha2
    pciBusID: "0000:00:06.0"
    ip:
      fw0: "172.16.3.100/32"
      fw1: "172.16.3.101/32"
  - name: net1
    pciBusID: "0000:00:07.0"
  - name: net2
    pciBusID: "0000:00:08.0"
```

4. Apply crds
```
kubectl apply -f cnv3/crds/pan-cn-mgmt-slot-crd.yaml
kubectl apply -f cnv3/crds/plugin-serviceaccount.yaml
```

5. Install Helm Chart to deploy the CN resources to EKS

```
cd ~/ps-aws-lab-cnf-eks
helm install mycn cnv3 --values eks-h.yaml
```

## ENIs and Secondary IPs

The interface attachment index order is very important to identify which ENI is being assigned to the corresponding DP pod. Unfortunately, the new EC2 console does not show the interface index order when viewing the instance. You can use this AWS CLI query to get a view of the interface order for the EKS nodes used for this deployment.

```
aws ec2 describe-instances --filters "Name=tag:aws:autoscaling:groupName,Values=lab-k8s-cn-v3-ng1" --query 'Reservations[].Instances[].{InstanceId: InstanceId, NetworkInterfaces: NetworkInterfaces[].{Id: NetworkInterfaceId, DeviceIndex: Attachment.DeviceIndex, PrimaryIp: PrivateIpAddress}}' --output table
```


## Interface Helper Function

1. Create this helper function that will find the appropriate interface ID based on the primary IP address of the EC2 Instance (EKS Nodes). This will be reference in later steps

```
function awsinstbyip  { aws ec2 describe-instances --region us-west-2 --filter Name=private-ip-address,Values=$1 | jq '.Reservations[0].Instances[0] | {"id":.InstanceId, "ni": .NetworkInterfaces | [.[] | {"di":.Attachment.DeviceIndex,"ip":.PrivateIpAddress,"eni":.NetworkInterfaceId} ] | sort_by(.di)}'; }
```


## Set secondary IP for HA2 Interface

We specified static IPs for the HA2 interfaces in the helm chart. Now we must make those same addresses available on the AWS ENI that is mapped to the corresponding HA2 pod interface. The ENIs of the nodes were created with dynamic IP addresses and we do not know which K8S node will be used to schedule the dp-0 and dp-1 pods. We will handle this by identifying which node each is running on and then add a secondary IP address to the ENIs to match the static mapping that was defined in the helm chart.

1. Identify which nodes each CN deployment is running on

```
kubectl get pods -n kube-system -l=app=pan-mgmt -o wide
```

2. Get the IP address of the corresponding node hosting both pods

```
kubectl get nodes -o wide
```

3. Set shell variable to reference the IP address of the Node that the corresponding CN pods are running on

```
dp0node=172.16.1.xxx
dp1node=172.16.1.xxx
```

4. Create secondary IP addresses for HA2 on the appropriate ENIs
```
aws ec2 assign-private-ip-addresses --region us-west-2 --allow-reassignment \
    --private-ip-addresses 172.16.3.100 \
    --network-interface-id $(awsinstbyip $dp0node | jq -r '.ni[1].eni')

aws ec2 assign-private-ip-addresses --region us-west-2  --allow-reassignment \
    --private-ip-addresses 172.16.3.101 \
    --network-interface-id $(awsinstbyip $dp1node | jq -r '.ni[1].eni')
```

## Secondary IPs for traffic

Next we will create secondary IP address for the Dataplane interfaces. These are the addresses that would be used for routing peers or next hop into the CNF. For the initial assignment, we will identify which node is hosting the currently active HA member and assign them to the interface. During HA failover, they will be moved to the other ndoe.

Find the K8S node hosting the active DP pod, we will save it into *nip* variable to avoid putting it into too many places.

These dataplane secondary IPs should be associated to the node that is hosting the *active* HA member. 

1. Start a PAN-OS shell to the management pods to verify which one is active.

```
kubectl -n kube-system exec -it pan-mp-mycn-sts-0-0 -- su admin
kubectl -n kube-system exec -it pan-mp-mycn-sts-1-0 -- su admin
```

In this case pod `pan-mp-mycn-sts-0-0` is active but it could be either.

![Screenshot 2022-09-18 160614](https://user-images.githubusercontent.com/43679669/190932266-b0197761-6666-46e2-9fdd-35fe20d31508.png)

2. Doublecheck what node the active HA pod is running on

```
kubectl get pods -n kube-system -l=app=pan-mgmt -o wide
```

![Screenshot 2022-09-18 161031](https://user-images.githubusercontent.com/43679669/190932247-b751da2b-e660-4c4c-866c-1bfe8135da91.png)

3. Get the IP address of the corresponding node hosting the currently active pod.

```
kubectl get nodes -o wide
```

![Screenshot 2022-09-18 161131](https://user-images.githubusercontent.com/43679669/190932230-72b134c3-a004-472d-98c8-6be2c3efe1d9.png)

4. Set the node IP variable which will be passed into the function to identify the appropriate ENI to add the secondary IP addresses to for eth1/2 and eth1/3 dataplane interfaces.

```
nip=172.16.1.xxx
aws ec2 assign-private-ip-addresses --region us-west-2  --allow-reassignment \
    --private-ip-addresses 172.16.4.199 \
    --network-interface-id $(awsinstbyip $nip | jq -r '.ni[2].eni')
aws ec2 assign-private-ip-addresses --region us-west-2  --allow-reassignment \
    --private-ip-addresses 172.16.5.199 \
    --network-interface-id $(awsinstbyip $nip | jq -r '.ni[3].eni')
```

## Routes for traffic

In addition to secondary IPs, AWS VPC routes can direct traffic directly to a dataplane ENI. These routes will also be updated during HA failover. For the initial setup we will create routes to the ENIs of the node that is hosting the currently active CN deployment.

1. Find the routing table associated with the multus subnets. Note we're using *nip* variable from the previous step

```
eksvpc=$(aws ec2 describe-vpcs --filters "Name=tag:Name,Values=vpc-cn-v3" --query 'Vpcs[0].VpcId' --output text)
rt=$(aws ec2 describe-route-tables --filters Name=association.main,Values=true Name=vpc-id,Values=$eksvpc --query 'RouteTables[0].RouteTableId' --output text)

echo $rt
```

2. Create routes pointed to the ENIs / DP interfaces

```
aws  ec2 create-route   --region us-west-2 --destination-cidr-block 172.17.4.0/25 \
    --route-table-id $rt \
    --network-interface-id $(awsinstbyip $nip | jq -r '.ni[3].eni')
aws  ec2 create-route   --region us-west-2 --destination-cidr-block 172.17.5.0/25 \
    --route-table-id $rt \
    --network-interface-id $(awsinstbyip $nip | jq -r '.ni[2].eni')
```

## Verify Routing

Now that the secondary IPs and routes are in place, traffic can be directed to the CNF using traditional AWS designs.

- BGP Peering to the secondary IP of the dataplane interfaces
- Route traffic from AWS VPCs directly to the the ENI of the dataplane interfaces

BGP Peering configuration was prepped for the CNs to the "multus" ubuntu VMs deployed in the VPC.

1. Validate BGP Peers are up on the active node

```
show routing protocol bgp summary
```

![Screenshot 2022-09-18 162827](https://user-images.githubusercontent.com/43679669/190932221-7af54c64-f7fb-4f8f-a4e5-3cf3e7d34435.png)

## Test Failover

During failover, three things are triggered in AWS to move IP addresses and routes to the EC2 Instance / EKS Node that is hosting the now-active CN.

- Move any secondary IPs on the dataplane ENIs to the corresponding ENIs of the other instance
- Reassociate any elastic IPs that are associated to the secondary dataplane IPs
- Update any routes in VPC route tables poitned to a dataplane ENI

1. Check current HA status

Make sure HA2 is active.

```
> show high-availability state
> show high-availability state-synchronization
> show plugins vm_series aws ha ips 
> show plugins vm_series aws ha state
> show system setting dpdk-pkt-io
```

2. Check which instance the secondary IPs are currently active

You can also check in the EC2 console by viewing the instances.

```
aws ec2 describe-instances --filters "Name=tag:aws:autoscaling:groupName,Values=lab-k8s-cn-v3-ng1" --query 'Reservations[].Instances[].{InstanceId: InstanceId, NetworkInterfaces: NetworkInterfaces[?Attachment.DeviceIndex > `1`].{Id: NetworkInterfaceId, DeviceIndex: Attachment.DeviceIndex, AllIps: PrivateIpAddresses[*].PrivateIpAddress}}' --output table
```

3. Suspend the active HA member to force failover

From the passive mgmt-pod cli

```
request high-availability state peer suspend
```

4. Validate IPs have been moved to the other instance

```
aws ec2 describe-instances --filters "Name=tag:aws:autoscaling:groupName,Values=lab-k8s-cn-v3-ng1" --query 'Reservations[].Instances[].{InstanceId: InstanceId, NetworkInterfaces: NetworkInterfaces[?Attachment.DeviceIndex > `1`].{Id: NetworkInterfaceId, DeviceIndex: Attachment.DeviceIndex, AllIps: PrivateIpAddresses[*].PrivateIpAddress}}' --output table
```

5. Validate BGP Peers are still active on the now-active member

```
show routing protocol bgp summary
```



## Extra

Useful Commands to use during lab activities

```
kubectl get pods -n kube-system -l=app=pan-ngfw -o wide
kubectl -n kube-system logs pan-dp-mycn-dep-0-xxxxxxxxx
kubectl -n kube-system exec -it pan-mp-mycn-sts-0-0 -- su admin
kubectl -n kube-system exec -it pan-mp-mycn-sts-0-0 -- bash
```